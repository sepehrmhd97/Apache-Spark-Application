{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57432768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0796bc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/02/22 19:11:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark_session = SparkSession.builder\\\n",
    "    .master(\"spark://192.168.2.70:7077\") \\\n",
    "    .appName(\"Sepehr_3B\")\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "    .config(\"spark.shuffle.service.enabled\", True)\\\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "    .config(\"spark.cores.max\", 4)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30572661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+----------+--------+-----------+--------------+-----------------+----+----+----------+-----+--------------------+-----+------+--------------+---------------------+-----------+---------+---------+------------------+-----------------+----------------------+\n",
      "|Ticket number|          Issue Date|Issue time|Meter Id|Marked Time|RP State Plate|Plate Expiry Date| VIN|Make|Body Style|Color|            Location|Route|Agency|Violation code|Violation Description|Fine amount| Latitude|Longitude|Agency Description|Color Description|Body Style Description|\n",
      "+-------------+--------------------+----------+--------+-----------+--------------+-----------------+----+----+----------+-----+--------------------+-----+------+--------------+---------------------+-----------+---------+---------+------------------+-----------------+----------------------+\n",
      "|   1103341116|2015-12-21T00:00:...|      1251|    null|       null|            CA|           200304|null|HOND|        PA|   GY|     13147 WELBY WAY|01521|     1|        4000A1|   NO EVIDENCE OF REG|         50|    99999|    99999|              null|             null|                  null|\n",
      "|   1103700150|2015-12-21T00:00:...|      1435|    null|       null|            CA|           201512|null| GMC|        VN|   WH|       525 S MAIN ST| 1C51|     1|        4000A1|   NO EVIDENCE OF REG|         50|    99999|    99999|              null|             null|                  null|\n",
      "|   1104803000|2015-12-21T00:00:...|      2055|    null|       null|            CA|           201503|null|NISS|        PA|   BK|       200 WORLD WAY|  2R2|     2|          8939|           WHITE CURB|         58|6439997.9|1802686.4|              null|             null|                  null|\n",
      "|   1104820732|2015-12-26T00:00:...|      1515|    null|       null|            CA|             null|null|ACUR|        PA|   WH|       100 WORLD WAY| 2F11|     2|           000|               17104h|       null|6440041.1|1802686.2|              null|             null|                  null|\n",
      "|   1105461453|2015-09-15T00:00:...|       115|    null|       null|            CA|           200316|null|CHEV|        PA|   BK|  GEORGIA ST/OLYMPIC|1FB70|     1|         8069A| NO STOPPING/STANDING|         93|    99999|    99999|              null|             null|                  null|\n",
      "|   1106226590|2015-09-15T00:00:...|        19|    null|       null|            CA|           201507|null|CHEV|        VN|   GY|  SAN PEDRO S/O BOYD|1A35W|     1|        4000A1|   NO EVIDENCE OF REG|         50|    99999|    99999|              null|             null|                  null|\n",
      "|   1106500452|2015-12-17T00:00:...|      1710|    null|       null|            CA|           201605|null|MAZD|        PA|   BL|     SUNSET/ALVARADO|00217|     1|          8070| PARK IN GRID LOCK ZN|        163|    99999|    99999|              null|             null|                  null|\n",
      "|   1106500463|2015-12-17T00:00:...|      1710|    null|       null|            CA|           201602|null|TOYO|        PA|   BK|     SUNSET/ALVARADO|00217|     1|          8070| PARK IN GRID LOCK ZN|        163|    99999|    99999|              null|             null|                  null|\n",
      "|   1106506402|2015-12-22T00:00:...|       945|    null|       null|            CA|           201605|null|CHEV|        PA|   BR|      721 S WESTLAKE| 2A75|     1|        8069AA|     NO STOP/STAND AM|         93|    99999|    99999|              null|             null|                  null|\n",
      "|   1106506413|2015-12-22T00:00:...|      1100|    null|       null|            CA|           201701|null|NISS|        PA|   SI|     1159 HUNTLEY DR| 2A75|     1|        8069AA|     NO STOP/STAND AM|         93|    99999|    99999|              null|             null|                  null|\n",
      "|   1106506424|2015-12-22T00:00:...|      1100|    null|       null|            CA|           201511|null|FORD|        TR|   WH|     1159 HUNTLEY DR| 2A75|     1|        8069AA|     NO STOP/STAND AM|         93|    99999|    99999|              null|             null|                  null|\n",
      "|   1106506435|2015-12-22T00:00:...|      1105|    null|       null|            CA|           201701|null|CHRY|        PA|   GO|     1159 HUNTLEY DR| 2A75|     1|        8069AA|     NO STOP/STAND AM|         93|    99999|    99999|              null|             null|                  null|\n",
      "|   1106506446|2015-12-22T00:00:...|      1110|    null|       null|            CA|           201511|null| BMW|        PA|   BK|      1200 W MIRAMAR| 2A75|     1|        4000A1|   NO EVIDENCE OF REG|         50|    99999|    99999|              null|             null|                  null|\n",
      "|   1106549754|2015-12-15T00:00:...|       825|    null|       null|            CA|           201607|null|PTRB|        TR|   BK|           4TH/STATE| CM96|     1|         8069A| NO STOPPING/STANDING|         93|    99999|    99999|              null|             null|                  null|\n",
      "|   1107179581|2015-12-27T00:00:...|      1055|    null|       null|            CA|           201605|null|TOYO|        PA|   BK|3100 N HOLLYRIDGE DR| null|    54|         8058L|         PREF PARKING|         68|    99999|    99999|              null|             null|                  null|\n",
      "|   1107179592|2015-12-27T00:00:...|      1200|    null|       null|            CA|           201602|null|MBNZ|        PA|   BK|   3115 N BERENDO DR| null|    54|         8058L|         PREF PARKING|         68|    99999|    99999|              null|             null|                  null|\n",
      "|   1107179603|2015-12-27T00:00:...|      1400|    null|       null|            CA|           201611|null|NISS|        PA|   WH| 3100 N BEACHWOOD DR| null|    54|         8058L|         PREF PARKING|         68|    99999|    99999|              null|             null|                  null|\n",
      "|   1107539823|2015-09-16T00:00:...|      2120|    null|       null|            CA|           201502|null|NISS|        PA| null|      BLAINE/11TH PL|1FB95|     1|        4000A1|   NO EVIDENCE OF REG|         50|    99999|    99999|              null|             null|                  null|\n",
      "|   1107539834|2015-09-16T00:00:...|      1045|    null|       null|            CA|             null|null|CHEV|        PA|   BK|  1246 S FIGUEROA ST| 1L20|     1|        8069AP|     NO STOP/STAND PM|         93|    99999|    99999|              null|             null|                  null|\n",
      "|   1107780811|2015-12-22T00:00:...|      1102|    null|       null|            CA|           201606|null|HOND|        PA|   BK|       PLATA/RAMPART|  2A1|     1|         8069B|           NO PARKING|         73|    99999|    99999|              null|             null|                  null|\n",
      "+-------------+--------------------+----------+--------+-----------+--------------+-----------------+----+----+----------+-----+--------------------+-----+------+--------------+---------------------+-----------+---------+---------+------------------+-----------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1. upload and show the DataFrame\n",
    "pc_df= spark_session.read.format(\"csv\").option(\"header\", \"true\").load(\"hdfs://192.168.2.70:9000/parking-citations.csv\")\n",
    "pc_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2493e7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ticket number: string (nullable = true)\n",
      " |-- Issue Date: string (nullable = true)\n",
      " |-- Issue time: string (nullable = true)\n",
      " |-- Meter Id: string (nullable = true)\n",
      " |-- Marked Time: string (nullable = true)\n",
      " |-- RP State Plate: string (nullable = true)\n",
      " |-- Plate Expiry Date: string (nullable = true)\n",
      " |-- VIN: string (nullable = true)\n",
      " |-- Make: string (nullable = true)\n",
      " |-- Body Style: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Route: string (nullable = true)\n",
      " |-- Agency: string (nullable = true)\n",
      " |-- Violation code: string (nullable = true)\n",
      " |-- Violation Description: string (nullable = true)\n",
      " |-- Fine amount: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Agency Description: string (nullable = true)\n",
      " |-- Color Description: string (nullable = true)\n",
      " |-- Body Style Description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pc_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca9c8918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==============================================>          (13 + 3) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the Parking Citation CSV file: 13077724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#3. Count the number of rows in the CSV file.\n",
    "rows_count = pc_df.count()\n",
    "print(f'Number of rows in the Parking Citation CSV file: {rows_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53d005ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in the Parking Citation CSV file: 16\n"
     ]
    }
   ],
   "source": [
    "#4. Count the number of partitions in the underlying RDD.\n",
    "partitions_count = pc_df.rdd.getNumPartitions()\n",
    "print(f'Number of partitions in the Parking Citation CSV file: {partitions_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1396896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Drop the columns VIN, Latitude and Longitude.\n",
    "pc_df = pc_df.drop(\"VIN\", \"Latitude\", \"Longitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d6ab373",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum fine amount: $ 1100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:=============================================>          (13 + 3) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fines with maximum fine amount:  626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#6. Find the maximum fine amount. How many fines have this amount?\n",
    "\n",
    "# convert 'fine amount' column to float\n",
    "pc_df = pc_df.withColumn(\"Fine amount\", col(\"Fine amount\").cast(\"float\"))\n",
    "\n",
    "# find the maximum fine amount\n",
    "max_fine_amount = pc_df.agg({\"Fine amount\": \"max\"}).collect()[0][0]\n",
    "print(\"Maximum fine amount: $\", max_fine_amount)\n",
    "\n",
    "# count the number of fines with this amount\n",
    "num_max_fines = pc_df.filter(col(\"Fine amount\") == max_fine_amount).count()\n",
    "print(\"Number of fines with maximum fine amount: \", num_max_fines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ce223f",
   "metadata": {},
   "source": [
    "# orderby method\n",
    "The orderBy method in PySpark is used to sort the data in a DataFrame or Dataset based on one or more columns. The method takes one or more column names as arguments, and returns a new DataFrame or Dataset sorted in ascending order based on the specified columns.\n",
    "\n",
    "Here is a step-by-step overview of how the orderBy method works:\n",
    "\n",
    "The method takes one or more column names as arguments, which specifies the columns to be used for sorting.\n",
    "\n",
    "If multiple columns are specified, the DataFrame is sorted first by the values in the first column, then by the values in the second column, and so on.\n",
    "\n",
    "By default, the sorting is performed in ascending order. If you want to sort in descending order, you can use the desc() method on the column, like this: df.orderBy(col(\"column_name\").desc()).\n",
    "\n",
    "The orderBy method returns a new DataFrame or Dataset that contains the same rows as the original, but with the rows sorted based on the specified columns.\n",
    "\n",
    "The orderBy method does not modify the original DataFrame or Dataset. Instead, it returns a new one that you can use for further processing.\n",
    "\n",
    "In summary, the orderBy method is a powerful tool for sorting data in PySpark, allowing you to easily sort your data by one or more columns in either ascending or descending order.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41855892",
   "metadata": {},
   "source": [
    "# wuthColumn() method\n",
    "withColumn() is a method in PySpark's DataFrame API that allows you to add a new column to an existing DataFrame or replace an existing column. The method takes two arguments:\n",
    "\n",
    "colName: The name of the new column you want to add or the name of the column you want to replace.\n",
    "col: The expression or value that you want to use to compute the values of the new column.\n",
    "\n",
    "DataFrame.withColumn(colName, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d95ec4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Show the top 20 most frequent vehicle makes, and their frequencies.\n",
    "\n",
    "def freq(column_name, n):\n",
    "    freq_df = pc_df.groupBy(column_name).count().orderBy(desc(\"count\"))\n",
    "    freq_df.show(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97c8a1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|Make|  count|\n",
      "+----+-------+\n",
      "|TOYT|2150768|\n",
      "|HOND|1479996|\n",
      "|FORD|1116235|\n",
      "|NISS| 945133|\n",
      "|CHEV| 892676|\n",
      "| BMW| 603092|\n",
      "|MERZ| 543298|\n",
      "|VOLK| 432030|\n",
      "|HYUN| 404917|\n",
      "|DODG| 391686|\n",
      "|LEXS| 368420|\n",
      "| KIA| 328155|\n",
      "|JEEP| 316300|\n",
      "|AUDI| 255395|\n",
      "|MAZD| 242344|\n",
      "|OTHR| 205546|\n",
      "| GMC| 184889|\n",
      "|INFI| 174315|\n",
      "|CHRY| 159948|\n",
      "|SUBA| 154640|\n",
      "+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "freq(\"Make\", 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6e06a0",
   "metadata": {},
   "source": [
    "# UDF\n",
    "UDF stands for User-Defined Function. In the context of PySpark, a UDF is a way to define a custom function that can be applied to a PySpark DataFrame or column. UDFs allow you to use your own code to perform operations on DataFrame columns that are not supported by built-in PySpark functions.\n",
    "\n",
    "When defining a UDF, you write a regular Python or Scala function and then register it with PySpark using the udf() function. Once registered, you can apply the UDF to a DataFrame column using the withColumn() method.\n",
    "\n",
    "UDFs are useful when you need to apply a custom transformation to a PySpark DataFrame or column, especially when the built-in PySpark functions are not sufficient for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5dcd73be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_color(color):\n",
    "    COLORS = {\n",
    "        'AL': 'Aluminum', 'AM': 'Amber', 'BG': 'Beige', 'BK': 'Black', 'BL': 'Blue',\n",
    "        'BN': 'Brown', 'BR': 'Brown', 'BZ': 'Bronze', 'CH': 'Charcoal', 'DK': 'Dark',\n",
    "        'GD': 'Gold', 'GO': 'Gold', 'GN': 'Green', 'GY': 'Gray', 'GT': 'Granite',\n",
    "        'IV': 'Ivory', 'LT': 'Light', 'OL': 'Olive', 'OR': 'Orange', 'MR': 'Maroon',\n",
    "        'PK': 'Pink', 'RD': 'Red', 'RE': 'Red', 'SI': 'Silver', 'SL': 'Silver',\n",
    "        'SM': 'Smoke', 'TN': 'Tan', 'VT': 'Violet', 'WT': 'White', 'WH': 'White',\n",
    "        'YL': 'Yellow', 'YE': 'Yellow', 'UN': 'Unknown'\n",
    "    }\n",
    "    return COLORS.get(color, color) # get(a,b) returns b as default value if it is not in the dictionary\n",
    "\n",
    "# Create UDF from the function\n",
    "map_color_udf = udf(map_color)\n",
    "\n",
    "# Apply UDF to create new column\n",
    "pc_df = pc_df.withColumn(\"color long\", map_color_udf(pc_df[\"Color\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0db96e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_color_brand(brand_name, n):\n",
    "    freq = pc_df.groupBy(\"Make\", \"color long\").count().filter(pc_df.Make == brand_name).orderBy(\"count\", ascending=False).take(n)\n",
    "    print(f'{n} Most Frequent Color in {brand_name}:')\n",
    "    for b, color, count in freq:\n",
    "        print(f'{color}:{count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "18cbfd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Most Frequent Color in TOYT:\n",
      "Gray:489697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "freq_color_brand('TOYT', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "40e252be",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8025a31a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
